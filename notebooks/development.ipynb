{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the library\n",
    "from zoraspeech.listener import Listener\n",
    "from zoraspeech.architectures.cnn.cnn import ConvModel\n",
    "from zoraspeech.interpreters.cnn.layer_visualizations import LightweightVisualizer\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "import torch as t\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Audio\n",
    "import random\n",
    "import glob\n",
    "#import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a listener\n",
    "listener = Listener(\n",
    "    model_architecture=ConvModel(), \n",
    "    model_weights='/Users/jo/Documents/zora/src/zoraspeech/weights/audrey_model_weights_2024-10-26.pth',\n",
    "    interpreter=LightweightVisualizer()\n",
    "    )\n",
    "listener.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the random file\n",
    "random_file = '/Users/jo/Documents/listening_and_speaking_machines/listening_machines/speech_digit_dataset/test_6.wav'\n",
    "\n",
    "recorded_audio, sample_rate = torchaudio.load(random_file)\n",
    "print(sample_rate)\n",
    "\n",
    "# visualize\n",
    "print(recorded_audio.shape)\n",
    "print(len(recorded_audio[0]))\n",
    "# pad the audio to the longest audio file length\n",
    "\n",
    "#recorded_audio = np.array(recorded_audio)[0]\n",
    "current_size = len(recorded_audio[0])\n",
    "\n",
    "longest_audio_file_length = 17916\n",
    "\n",
    "if current_size > longest_audio_file_length:\n",
    "    print(\"Audio is too long, truncating\")\n",
    "    padded_audio = recorded_audio[:, :longest_audio_file_length]\n",
    "    current_size = longest_audio_file_length\n",
    "else:\n",
    "    print(\"Audio is short enough, padding\")\n",
    "\n",
    "    pad_size = longest_audio_file_length - current_size\n",
    "    print(\"Pad size: \", pad_size)\n",
    "    left_pad = pad_size // 2\n",
    "    print(\"Left pad: \", left_pad)\n",
    "    right_pad = pad_size - left_pad\n",
    "    print(\"Right pad: \", right_pad)\n",
    "    padded_audio = np.pad(recorded_audio[0], (left_pad, right_pad), mode='constant')\n",
    "    print(\"Padded audio shape: \", padded_audio.shape)\n",
    "    print(padded_audio.shape)\n",
    "\n",
    "display(Audio(padded_audio, rate=sample_rate))\n",
    "\n",
    "padded_audio_array = np.array(padded_audio)\n",
    "audio = t.tensor([padded_audio_array])\n",
    "\n",
    "# display spectrogram\n",
    "spec = torchaudio.transforms.MelSpectrogram()(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have listener object \"listen\" to audio spectrogram\n",
    "listener.listen(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the activation\n",
    "listener.interpreter.interpret(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "listener.interpret.list()\n",
    "\n",
    "listener.interpret(\"cnn_visualization\")\n",
    "\n",
    "--> generate activation maps from cnn model\n",
    "\n",
    "\n",
    "listener.interpreter.list()\n",
    "listener.interpreter.interpret(\"cnn_visualization\")\n",
    "\n",
    "--> generate activation maps from cnn model + sonification\n",
    "\n",
    "listener.interpreter.interpret(\"mech_interp\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoraspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
